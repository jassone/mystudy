## I/O模型

**IO，即输入/输出**，到底谁是输入？谁是输出呢？

## 一、IO基础知识

### 1、计算机角度的IO

从计算机架构来说，**涉及计算机核心与其他设备间数据迁移的过程，就是IO**。如磁盘IO，就是从磁盘读取数据到内存，这算一次输入，对应的，将内存中的数据写入磁盘，就算输出。这就是IO的本质。

![59fc1db4af9f2b4fe.png](https://pic.imgdb.cn/item/62dd1077f54cd3f93768c88f.png)

### 2、操作系统的IO

操作系统负责计算机的资源管理和进程的调度。我们电脑上跑着的应用程序，**其实是需要经过操作系统，**才能做一些特殊操作，如磁盘文件读写、内存的读写等等。因为这些都是比较危险的操作，不可以由应用程序乱来，只能交给底层操作系统来。也就是说，你的应用程序要把数据写入磁盘，**只能通过调用操作系统开放出来的API来操作。**

我们应用程序是跑在用户空间的，它不存在实质的IO过程，真正的IO是在**操作系统**执行的。

即应用程序的IO操作分为两种动作：**IO调用和IO执行**。

- IO调用是由进程（应用程序的运行态）发起。
- 而IO执行是**操作系统内核**的工作。

此时所说的IO是应用程序对操作系统IO功能的一次触发，即IO调用。

### 3、操作系统的一次IO过程

##### a) 应用程序发起的一次IO操作，包含两个对象和两个阶段：

两个对象

- 用户进程（线程）Process（Thread）。
- 内核对象（kernel）。

两个操作

- IO调用：应用程序进程向操作系统**内核**发起调用。
- IO执行：操作系统内核完成IO操作。

##### b) 操作系统内核完成IO操作，包括两个过程：

- 准备数据阶段：内核等待I/O设备准备好数据，将数据加载到内核缓存（数据加载到操作系统）。
- 拷贝数据阶段：将数据从内核缓冲区拷贝到用户进程(线程)缓冲区（从操作系统复制到应用中）。

对于socket而言：

- 第一阶段通常等待网络上的数据分组到达，然后被复制到内核的某个缓冲区。
- 第二阶段数据从内核的缓冲区复制到应用进程的缓冲区。

![39f6c3f2faed0447.png](https://pic.imgdb.cn/item/62dcf2fdf54cd3f937badd6c.png)

##### c) IO本质

**IO就是把进程的内部数据转移到外部设备，或者把外部设备的数据迁移到进程内部**。外部设备一般指硬盘、**socket通讯的网卡**。一个完整的**IO过程**包括以下几个步骤：

- 应用程序进程向操作系统发起**IO调用请求**。
- 操作系统**准备数据**，把IO外部设备的数据，加载到**内核缓冲区。**
- 操作系统拷贝数据，即将内核缓冲区的数据，拷贝到用户进程缓冲区。

## 一、其他几个概念

### 1、流是什么

**IO操作即对流进行读写。网络IO的本质就是socket流的读写。**

流是可以进行I/o操作的内核对象，比如文件、管道、套接字等。
流的入口：文件描述符(fd)，socket描述符(fd)等。

![20220227095546.jpg](https://pic.imgdb.cn/item/621ada2a2ab3f51d91d8fd17.jpg)

### 3、同步和异步
**同步和异步的概念描述的是用户线程与内核的交互方式。**也可理解为被被调用者(操作系统)的角度来说。

**同步和异步是针对应用程序来说的，关注的是程序中间的协作关系。**

- 同步是用户进程触发IO操作并等待或轮询的去查看是否就绪，请求方从发起请求到数据最后完成的这一段过程中都需要自己参与。
- 而异步是指用户进程触发IO操作以后便开始做自己的事情，并当IO操作已经完成的时候会收到IO完成的通知，需要CPU支持。

异步是IO最理想的模型：cpu的原则是有必要了才会参与，也不浪费也不怠慢。

### 4、阻塞和非阻塞
**阻塞和非阻塞的概念描述的是用户线程调用内核IO操作的方式。**也可理解为调用者(程序)角度来说。

**阻塞和非阻塞关注的是单个进程的执行状态。**

阻塞，可以看做是进程被休息了，cpu处理其他事情了。

非阻塞，可以理解为，将大的整片时间的阻塞分成N多的小的阻塞，所以进程不断地有机会被cpu光顾，理论上可以做点其他事情，但是cpu会很大概率因socket没有数据而空转，浪费资源。

阻塞-等待：比如只有读，没有写，则会阻塞等待。推荐优先使用阻塞等待模式。
非阻塞-忙轮询：比如while(1){},占用cpu、系统资源。

* 区分阻塞和非阻塞：看read/write函数调用是否立即返回，是则是非阻塞(然后可以轮询检测)。
* 区分同步和异步IO:看read/write操作是应用程序完成，还是操作系统完成，再通知应用程序。若是操作系统完成则是异步IO.

### 5、IO模型分类

- 内存IO。
- 网络I/O，也就是通过网络进行数据的拉取和输出。
- 磁盘I/O,主要是对磁盘进行读写工作。

## 二、五种IO模型

### 1、同步阻塞IO(BIO)

阻塞IO模型，简称**BIO**，Blocking IO。

linux系统的read和write函数，在调用的时候会被阻塞，直到数据读取完成，或者写入成功。

![df9b2fa3b87b275.png](https://pic.imgdb.cn/item/62dd11fef54cd3f937719dd4.png)

阻塞IO比较经典的应用就是**阻塞socket、Java BIO**。

### 2、同步非阻塞IO(NIO)

非阻塞IO模型，简称**NIO**，Non-Blocking IO。

默认创建的socket都是阻塞的，**非阻塞IO要求socket被设置为NONBLOCK**。即和同步阻塞IO的API是一样的，只是打开fd的时候带有O_NONBLOCK参数，于是当调用read和write函数的时候，如果没有准备好数据，会立即返回-1，**不会阻塞，这种情况下需要不断轮询查看状态。**

![cead812adf54759c.png](https://pic.imgdb.cn/item/62dd1275f54cd3f937747771.png)

非阻塞IO的流程如下

- 应用进程向操作系统内核，发起recvfrom(类似于标准的read())读取数据。
- 操作系统内核数据没有准备好，立即返回EWOULDBLOCK错误码。
- 应用程序进程轮询调用，继续向操作系统内核发起recvfrom读取数据。
- 操作系统内核数据准备好了，从内核缓冲区拷贝到用户空间。
- 完成调用，返回成功提示。

### 3、同步阻塞IO-IO多路复用(IO Multiplexing)---同步IO

**经典的Reactor设计模式。**多路复用一般都是用于网络IO。

前面两种IO都只能用于简单的客户端开发，但对于服务器来说，需要处理很多的fd（连接数可以达到几十万或者百万）。如果使用同步阻塞IO，要处理那么多的fd需要开非常多的线程，每个线程处理一个fd；如果使用同步非阻塞IO，就需要用应用程序轮询这么大规模的fd。

两种办法都不行，于是就有了IO多路复用，这也是最常见的一种。

linux中，有三种IO多路复用的办法：select，poll，**epoll(用的最多，主流)**

##### a) select

```c
int select(int maxfdp1, fd_set *readset, fd_set *writeset, fd_set *exceptset,struct timeval *timeout);
```
![rtrt4a.png](https://pic.imgdb.cn/item/62dd1500f54cd3f93783963c.png)

##### b) poll

```c
int poll(struct pollfd *fds, nfds_t nfds, int timeout);
struct pollfd{
	int fd;			//文件描述符
	short events;	//等待的事件
	short revents;	//实际发生的事件
}
```
通过上面函数会发现，select、poll每次调用都需要把应用程序fd的数组传进去，这个fd的数组每次都要在用户态和内核态之间传递，影响效率。

##### c) epoll

![d628b3259b5b9e5.png](https://pic.imgdb.cn/item/62dd1593f54cd3f937872306.png)

![cacb9f37.png](https://pic.imgdb.cn/item/62dca899f54cd3f93715ff7a.png)

```c
//该函数生成一个epoll专用的文件描述符(epoll的句柄)，size用来告诉内核监听的数目一共有多少。但不是准确的数字。
// 同时底层创建一个红黑树和就绪链表，红黑树存储所监控的文件描述符的节点数据，就绪链表存储就绪的文件描述符的节点数据。
//只是告诉内核，计划监听多少个fd。实际通过epoll_ctl添加的fd数目可能大于这个值。
int epoll_create（int size）
```

```c
// 该函数用于控制某个文件描述符上的事件，将一个fd增/删/改到epfd里，对应的事件也即读/写。
// 首先判断红黑树中是否存在，如果不存在，插入数据，并告知内核注册回调函数（当文件描述就绪时通过网卡驱动触发），数据就绪后将事件添加到就绪队列中。
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
```

```c
//该函数用于轮询I/O事件的发生； 检查链表，并将数据拷贝到用户空间（两者维护的是片共享内存），最后清空链表。
//多长时间去轮询一次， timeout=-1表示如果IO没有数据，一直阻塞。 正数表示超时时间
//返回几个有数据可读或可写的IO
int epoll_wait(int epfd,struct epoll_event *events,int maxevents,int timeout)
```

##### epoll的LT和ET模式
* LT：水平触发或条件触发
    读缓冲区只要不为空，就会一直触发读事件；
    写缓冲只要不满，就会一直触发写实际；

* ET：边缘触发或状态触发
    读缓冲区的状态，从空转为非空的时候触发一次(可读)；
    写缓冲区的状态，从满转为非满的时候触发一次(可写)

注意：
* 对于LT模式，要避免‘写的死循环’问题，即写缓冲区为满的概率很小，所以当用户注册了写事件却没有数据要写时，它会一直触发，因此在LT模式下写完数据一定要取消写事件。

* 对于ET模式，要避免‘short read’问题：比如用户收到100个字节，它触发一次，但用户只读到50个字节，剩下的50个字节不读，它也不会再次触发。因此在ET模式下，一定要把‘读缓冲区’的数据一次性读完。

在实际开发中，一般倾向于LT，这也是默认的模式。java NIO(New IO库)用的也是epoll的LT模式。因为ET容易漏事件，一次触发如果没有处理好，就没有第二次机会了。虽然LT重复触发可能有少许的性能消耗，但代码写起来更安全。

##### d) IO多路复用伪代码
```c
while(true) {
    select(...) 或者 poll(...) 或者 epoll_wait(...)
    if(某个fd有读事件) {
        read(fd,...)
    }
    if(某个fd有写事件) {
        write(fd,...)
    }
}
```

整个epoll的过程分为三个步骤：
* 事件注册，通过epoll_ctl实现。
对于服务器而言，是accept、read、write三种事件
对于客户端而言，是connect、read、write三种事件
* 轮询这三个事件是否就绪，通过epoll_wait实现，有事件发生，该函数返回。
* 事件就绪，执行实际的IO操作，通过accept/read/write实现

解释一下什么是‘事件就绪’
* read事件就绪：远程有新数据来了，socket读取缓冲区里的数据，需要调用read函数处理。
* write事件就绪：本地的socket写缓冲区是否可写，如果写缓冲区没有满，则一直是可写的，write事件一直是就绪的，可以调用write函数。只有当遇到发送大文件的场景，socket写缓冲区被占满时，write事件才不是就绪状态。
* accept事件就绪：有新的连接进入，需要调用accept函数处理。

##### e) IO多路复用总结

相比于阻塞IO模型， IO多路复用只是多了一个select/poll/epoll函数。select/poll/epoll函数会不断地轮询自己所负责的文件描述符/套接字的到达状态，当某个套接字就绪时，就对这个套接字进行处理。

- select,poll复杂度O（n）,epoll复杂度O（1）。
- select/poll/epoll负责轮询等待，recvfrom负责拷贝。
- 当用户进程调用该select，select会监听所有注册好的IO，
- **如果所有IO都没注册好，调用进程就阻塞(相当于没有准备好的IO)**。

**对于客户端来说，一般感受不到阻塞，因为请求来了，可以用放到线程池里执行；但对于执行select/poll/epoll的操作系统而言，是阻塞的，需要阻塞地等待某个套接字变为可读。**

##### IO多路复用阻塞在哪，复用在哪？

- IO多路复用是阻塞在select/poll/epoll这类系统调用上的，实际上read时也是阻塞的。
- IO多路复用复用的是执行select/poll/epoll的线程。

##### select/poll/epoll区别

![c93b5d6b84356895.png](https://pic.imgdb.cn/item/62dd176bf54cd3f937929f97.png)

### 四、同步非阻塞-信号驱动IO模型

**当进程发起一个IO操作，会向内核注册一个信号处理函数，然后进程返回不阻塞；当内核数据就绪时会发送一个信号给进程(操作系统回调到用户态的回调函数)，进程便在信号处理函数中调用IO读取数据(该阶段阻塞)。**

Linux使用SIGIO信号(signalIO)来实现**异步IO**通知机制。

该模型也分为两个阶段：

- 数据准备阶段：未阻塞，当数据准备完成之后，会主动的通知用户进程数据已经准备完成，对用户进程做一个回调。
- 数据拷贝阶段：阻塞用户进程，等待数据拷贝。

![20200605193656805.png](https://pic.imgdb.cn/item/62dc909bf54cd3f937b27b98.png)



- 优势：进程没有收到SIGIO信号之前，不被阻塞，可以做其他事情。

- 劣势：

  - 其实在Unix中，信号是一个被过度设计的机制(这句话来自知乎大神,有待考究)
  - 信号I/O在大量IO操作时可能会因为**信号队列溢出**导致没法通知——这个是一个非常严重的问题。

  - 当数据量变大时，信号产生太频繁，性能会非常低。内核需要不断的把数据复制到用户态。

应用层实现信号驱动I/O

```c
void func(int signo)
{
	读文件的操作；
}

fd = open("/dev/hellodev",O_RDWR);

fcntl(fd,F_SETOWN,getpid());

flage=fcntl(fd,F_GETFL);
fcntl(fd,F_SETFL,flage|FASYNC);

signal(SIGIO,func);
```

### 五、异步非阻塞IO(AIO)

异步肯定就是非阻塞了啊。

**经典的Proactor设计模式。**比如windows系统中的IOCP，这是一种**真正意义上的异步IO。**

**所谓异步IO，是指读写都是由操作系统完成的，然后通过回调函数或者某种其他通信机子通知应用程序。**

大白话描述的话，**就是用户进程发起系统调用后，立刻就可以开始去做其他的事情，然后直到I/O数据准备好并复制完成后，内核会给用户进程发送通知，告诉用户进程操作已经完成了。**

![71cf3bc79f3df8dc46e8fa82451028ea.png](https://pic.imgdb.cn/item/62dd1892f54cd3f9379978f0.png)

![d16a887ba0d2408b1.png](https://pic.imgdb.cn/item/62dd1acff54cd3f937a73614.png)

![b1e2bcf5c0.png](https://pic.imgdb.cn/item/62dce2b8f54cd3f93754963d.png)

具体流程

- 当用户进程发出aio_read操作之后，就立刻可以去做其它的事。
- 此时kernel收到read信号后，会立刻返回结果，所以不会对用户进程block。
- 之后，kernel会等待数据准备完毕，然后将数据拷贝到内存中。
- 完成之后，kernel会给用户进程发送signal信号，表示数据已经read操作完毕。整个过程无阻塞状态的发生。

特点：

- 异步I/O执行的两个阶段都不会阻塞读写操作，由内核完成。
- 完成后内核将数据放到指定的缓冲区，通知应用程序来取。

## 三、上层网络框架封装的网络IO模型

网络输入操作的两个阶段：

- 等待网络数据到达网卡→把数据读取到内核缓冲区。
- 从内核缓冲区复制数据到进程空间。

### 1、C++跨平台的网络库:asio,异步IO

AISO的‘异步’，是‘真异步’，还是‘假异步’？
* 在linux系统上封装的是epoll，不算真异步，只是IO多路复用
* 在windows系统上封装的是IOCP,是真异步

### 2、linux下java:NIO，Netty，epoll
java NIO和Linux epoll API对比：几乎一样

|  | java NIO| epoll |
| --- | --- |--- |
| 注册 |channel.register(selector,XXX) selectKey.interOps = XXX| epoll_ctr(...)|
|轮询| selector.poll()| epoll.wait(...) |
|实际IO操作| channel.accept channel.read channel.write| accept read write |

### 3、2个网络io的设计模式
操作系统的网络IO模型设计和上层网络框架的网络io模型的设计，用的都是这两种设计模式之一。
* Reactoer模式：主动模式，是指应用程序不断地轮询，询问操作系统或者网络框架、IO是否就绪。
    linux系统下的select/poll/epoll,java中的NIO都属于这种模式。在这种模式下，实际的IO操作还是应用程序执行的。

    好处：更加方便网络操作与业务进行分离。

* Proactor模式：被动模式，应用程序 把read和write函数操作全部交给操作系统或者网络框架，实际的IO操作由操作系统或网络框架完成，之后再回调应用程序。asio库就是典型的Proactor模型。

## 四、应用程序的网络IO于线程模型
### 1、普通网络IO模型
![20220226092142.jpg](https://pic.imgdb.cn/item/621980bf2ab3f51d91340817.jpg)

* 监听线程：负责accept事件的注册和处理。和每一个新进来的客户端建立socket连接，然后把socket连接移交给IO线程，完成任务，继续监听新的客户端。
* IO线程：负责每个socket连接上面read/write事件的注册和实际的socket的读写。把request放入request队列，交由worker线程处理。
* worker线程:纯碎的业务线程，没有socket读写操作。对request队列进行处理，生成response队列，然后写入response队列，由IO线程再回复给客户端。

##### a) 为什么叫做半同步半异步？
1-n是多路io复用 （epoll），异步 (包括请求接受，处理，解码，返回) 。worker是同步的（比如同步掉用数据库，等）。

##### b) 上面模型的N、M取值分别多大合适？
N：cpu的核数
M:M=cpu核数/(cpu时间/(cpu时间+IO时间))
比如cpu时间和IO时间为1:1，则线程为2*cpu核数

### 2、tomact 6的网络IO模型
![F33F6894-9C39-4A47-8BAA-B8990A0EE40B.png](https://pic.imgdb.cn/item/6219df2f2ab3f51d910737ec.png)
IO线程只负责read/write事件的注册和监听，执行了epoll里面的前两个阶段，第三个阶段是在worker线程里面做的。IO线程监听到一个socket连接上有读事件，于是把socket移交给worker线程，worker线程读出数据，处理完业务逻辑，直接返回给客户端。

* IO线程和worker线程之间交互，不再需要一来一回两个队列，直接是一个socket集合。
* worker线程不会被IO阻塞，是因为IO线程检测到读事件就绪之后，才把socket交给worker线程处理。

### 3、实际应用
实际应用中应用程序不需要自己开线程池，比如Netty框架内部已经把1，N，M 3个对应的线程池准备好了。

## 五、总结

### 1、阻塞阶段

- 阻塞IO、多路复用IO是两个阶段都处于阻塞状态。
- 非阻塞IO、信号驱动IO是在第二阶段从kernel读取数据时处于阻塞状态。
- 异步IO整个过程都未处于阻塞状态。

### 2、阻塞程度

阻塞IO > 非阻塞IO > 多路复用IO > 信号驱动IO > 异步IO ，并且效率由低到高。

### 3、阻塞、非阻塞、同步、异步IO划分

![3da870108418e392.png](https://pic.imgdb.cn/item/62dd1bedf54cd3f937adf537.png)

![56eafbf98002009e.jpeg](https://pic.imgdb.cn/item/62dd1c64f54cd3f937b0bb1e.jpg)

### 4、伪异步IO模型

伪异步IO模型采用线程池方式，但是底层仍然使用同步阻塞方式，限制了最大连接数。

## 七、相关wiki

* 框架篇：见识一下linux高性能网络IO+Reactor模型 https://juejin.cn/post/6892687008552976398 todo
